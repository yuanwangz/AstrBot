import sys
import traceback
import typing as T
from .base import BaseAgentRunner, AgentResponse, AgentResponseData, AgentState
from ...context import PipelineContext
from astrbot.core.provider.provider import Provider
from astrbot.core.platform.astr_message_event import AstrMessageEvent
from astrbot.core.message.message_event_result import (
    MessageChain,
)
from astrbot.core.provider.entities import (
    ProviderRequest,
    LLMResponse,
    ToolCallMessageSegment,
    AssistantMessageSegment,
    ToolCallsResult,
)
from mcp.types import (
    TextContent,
    ImageContent,
    EmbeddedResource,
    TextResourceContents,
    BlobResourceContents,
)
from astrbot.core.star.star_handler import EventType
from astrbot import logger

if sys.version_info >= (3, 12):
    from typing import override
else:
    from typing_extensions import override


# TODO:
# 1. å¤„ç†å¹³å°ä¸å…¼å®¹çš„å¤„ç†å™¨


class ToolLoopAgent(BaseAgentRunner):
    def __init__(
        self, provider: Provider, event: AstrMessageEvent, pipeline_ctx: PipelineContext
    ) -> None:
        self.provider = provider
        self.req = None
        self.event = event
        self.pipeline_ctx = pipeline_ctx
        self._state = AgentState.IDLE
        self.final_llm_resp = None
        self.streaming = False

    @override
    async def reset(self, req: ProviderRequest, streaming: bool) -> None:
        self.req = req
        self.streaming = streaming
        self.final_llm_resp = None
        self._state = AgentState.IDLE

    def _transition_state(self, new_state: AgentState) -> None:
        """è½¬æ¢ Agent çŠ¶æ€"""
        if self._state != new_state:
            logger.debug(f"Agent state transition: {self._state} -> {new_state}")
            self._state = new_state

    async def _iter_llm_responses(self) -> T.AsyncGenerator[LLMResponse, None]:
        """Yields chunks *and* a final LLMResponse."""
        if self.streaming:
            stream = self.provider.text_chat_stream(**self.req.__dict__)
            async for resp in stream:  # type: ignore
                yield resp
        else:
            yield await self.provider.text_chat(**self.req.__dict__)

    @override
    async def step(self):
        """
        Process a single step of the agent.
        This method should return the result of the step.
        """
        if not self.req:
            raise ValueError("Request is not set. Please call reset() first.")

        # å¼€å§‹å¤„ç†ï¼Œè½¬æ¢åˆ°è¿è¡ŒçŠ¶æ€
        self._transition_state(AgentState.RUNNING)
        llm_resp_result = None

        async for llm_response in self._iter_llm_responses():
            assert isinstance(llm_response, LLMResponse)
            if llm_response.is_chunk:
                if llm_response.result_chain:
                    yield AgentResponse(
                        type="streaming_delta",
                        data=AgentResponseData(chain=llm_response.result_chain),
                    )
                else:
                    yield AgentResponse(
                        type="streaming_delta",
                        data=AgentResponseData(
                            chain=MessageChain().message(llm_response.completion_text)
                        ),
                    )
                continue
            llm_resp_result = llm_response
            break  # got final response

        if not llm_resp_result:
            return

        # å¤„ç† LLM å“åº”
        llm_resp = llm_resp_result

        if llm_resp.role == "err":
            # å¦‚æžœ LLM å“åº”é”™è¯¯ï¼Œè½¬æ¢åˆ°é”™è¯¯çŠ¶æ€
            self.final_llm_resp = llm_resp
            self._transition_state(AgentState.ERROR)
            yield AgentResponse(
                type="err",
                data=AgentResponseData(
                    chain=MessageChain().message(
                        f"LLM å“åº”é”™è¯¯: {llm_resp.completion_text or 'æœªçŸ¥é”™è¯¯'}"
                    )
                ),
            )

        if not llm_resp.tools_call_name:
            # å¦‚æžœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè½¬æ¢åˆ°å®ŒæˆçŠ¶æ€
            self.final_llm_resp = llm_resp
            self._transition_state(AgentState.DONE)

            # æ‰§è¡Œäº‹ä»¶é’©å­
            if await self.pipeline_ctx.call_event_hook(
                self.event, EventType.OnLLMResponseEvent, llm_resp
            ):
                return

        # è¿”å›ž LLM ç»“æžœ
        if llm_resp.result_chain:
            yield AgentResponse(
                type="llm_result",
                data=AgentResponseData(chain=llm_resp.result_chain),
            )
        elif llm_resp.completion_text:
            yield AgentResponse(
                type="llm_result",
                data=AgentResponseData(
                    chain=MessageChain().message(llm_resp.completion_text)
                ),
            )

        # å¦‚æžœæœ‰å·¥å…·è°ƒç”¨ï¼Œè¿˜éœ€å¤„ç†å·¥å…·è°ƒç”¨
        if llm_resp.tools_call_name:
            tool_call_result_blocks = []
            for tool_call_name in llm_resp.tools_call_name:
                yield AgentResponse(
                    type="tool_call",
                    data=AgentResponseData(
                        chain=MessageChain().message(f"ðŸ”¨ è°ƒç”¨å·¥å…·: {tool_call_name}")
                    ),
                )
            async for result in self._handle_function_tools(self.req, llm_resp):
                if isinstance(result, list):
                    tool_call_result_blocks = result
                elif isinstance(result, MessageChain):
                    yield AgentResponse(
                        type="tool_call_result",
                        data=AgentResponseData(chain=result),
                    )
            # å°†ç»“æžœæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
            tool_calls_result = ToolCallsResult(
                tool_calls_info=AssistantMessageSegment(
                    role="assistant",
                    tool_calls=llm_resp.to_openai_tool_calls(),
                    content=llm_resp.completion_text,
                ),
                tool_calls_result=tool_call_result_blocks,
            )
            self.req.append_tool_calls_result(tool_calls_result)

    async def _handle_function_tools(
        self,
        req: ProviderRequest,
        llm_response: LLMResponse,
    ) -> T.AsyncGenerator[MessageChain | list[ToolCallMessageSegment], None]:
        """å¤„ç†å‡½æ•°å·¥å…·è°ƒç”¨ã€‚"""
        tool_call_result_blocks: list[ToolCallMessageSegment] = []
        logger.info(f"Agent ä½¿ç”¨å·¥å…·: {llm_response.tools_call_name}")

        # æ‰§è¡Œå‡½æ•°è°ƒç”¨
        for func_tool_name, func_tool_args, func_tool_id in zip(
            llm_response.tools_call_name,
            llm_response.tools_call_args,
            llm_response.tools_call_ids,
        ):
            try:
                if not req.func_tool:
                    return
                func_tool = req.func_tool.get_func(func_tool_name)
                if func_tool.origin == "mcp":
                    logger.info(
                        f"ä»Ž MCP æœåŠ¡ {func_tool.mcp_server_name} è°ƒç”¨å·¥å…·å‡½æ•°ï¼š{func_tool.name}ï¼Œå‚æ•°ï¼š{func_tool_args}"
                    )
                    client = req.func_tool.mcp_client_dict[func_tool.mcp_server_name]
                    res = await client.session.call_tool(func_tool.name, func_tool_args)
                    if not res:
                        continue
                    if isinstance(res.content[0], TextContent):
                        tool_call_result_blocks.append(
                            ToolCallMessageSegment(
                                role="tool",
                                tool_call_id=func_tool_id,
                                content=res.content[0].text,
                            )
                        )
                        yield MessageChain().message(res.content[0].text)
                    elif isinstance(res.content[0], ImageContent):
                        tool_call_result_blocks.append(
                            ToolCallMessageSegment(
                                role="tool",
                                tool_call_id=func_tool_id,
                                content="è¿”å›žäº†å›¾ç‰‡(å·²ç›´æŽ¥å‘é€ç»™ç”¨æˆ·)",
                            )
                        )
                        yield MessageChain().base64_image(res.content[0].data)
                    elif isinstance(res.content[0], EmbeddedResource):
                        resource = res.content[0].resource
                        if isinstance(resource, TextResourceContents):
                            tool_call_result_blocks.append(
                                ToolCallMessageSegment(
                                    role="tool",
                                    tool_call_id=func_tool_id,
                                    content=resource.text,
                                )
                            )
                            yield MessageChain().message(resource.text)
                        elif (
                            isinstance(resource, BlobResourceContents)
                            and resource.mimeType
                            and resource.mimeType.startswith("image/")
                        ):
                            tool_call_result_blocks.append(
                                ToolCallMessageSegment(
                                    role="tool",
                                    tool_call_id=func_tool_id,
                                    content="è¿”å›žäº†å›¾ç‰‡(å·²ç›´æŽ¥å‘é€ç»™ç”¨æˆ·)",
                                )
                            )
                            yield MessageChain().base64_image(res.content[0].data)
                        else:
                            tool_call_result_blocks.append(
                                ToolCallMessageSegment(
                                    role="tool",
                                    tool_call_id=func_tool_id,
                                    content="è¿”å›žçš„æ•°æ®ç±»åž‹ä¸å—æ”¯æŒ",
                                )
                            )
                            yield MessageChain().message("è¿”å›žçš„æ•°æ®ç±»åž‹ä¸å—æ”¯æŒã€‚")
                else:
                    logger.info(f"ä½¿ç”¨å·¥å…·ï¼š{func_tool_name}ï¼Œå‚æ•°ï¼š{func_tool_args}")
                    # å°è¯•è°ƒç”¨å·¥å…·å‡½æ•°
                    wrapper = self.pipeline_ctx.call_handler(
                        self.event, func_tool.handler, **func_tool_args
                    )
                    async for resp in wrapper:
                        if resp is not None:
                            # Tool è¿”å›žç»“æžœ
                            tool_call_result_blocks.append(
                                ToolCallMessageSegment(
                                    role="tool",
                                    tool_call_id=func_tool_id,
                                    content=resp,
                                )
                            )
                            yield MessageChain().message(resp)
                        else:
                            # Tool ç›´æŽ¥è¯·æ±‚å‘é€æ¶ˆæ¯ç»™ç”¨æˆ·
                            # è¿™é‡Œæˆ‘ä»¬å°†ç›´æŽ¥ç»“æŸ Agent Loopã€‚
                            self._transition_state(AgentState.DONE)
                            if res := self.event.get_result():
                                if res.chain:
                                    yield MessageChain(chain=res.chain)

                self.event.clear_result()
            except Exception as e:
                logger.warning(traceback.format_exc())
                tool_call_result_blocks.append(
                    ToolCallMessageSegment(
                        role="tool",
                        tool_call_id=func_tool_id,
                        content=f"error: {str(e)}",
                    )
                )

        # å¤„ç†å‡½æ•°è°ƒç”¨å“åº”
        if tool_call_result_blocks:
            yield tool_call_result_blocks

    def done(self) -> bool:
        """æ£€æŸ¥ Agent æ˜¯å¦å·²å®Œæˆå·¥ä½œ"""
        return self._state in (AgentState.DONE, AgentState.ERROR)

    def get_final_llm_resp(self) -> LLMResponse | None:
        return self.final_llm_resp
